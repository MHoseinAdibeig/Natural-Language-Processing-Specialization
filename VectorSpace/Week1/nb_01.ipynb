{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preprocessing**\n",
    "\n",
    "In this lab, we will be exploring how to preprocess tweets for sentiment analysis. We will provide a function for preprocessing \n",
    "tweets during this week's assignment, but it is still good to know what is going on under the hood. By the end of this lecture,\n",
    "you will see how to use the NLTK package to perform a preprocessing pipeline for Twitter datasets.\n",
    "\n",
    "**Setup**\n",
    "\n",
    "You will be doing sentiment analysis on tweets in the first two weeks of this course. To help with that, we will be using the Natural Language Toolkit (NLTK) package, an open-source Python library for natural language processing. It has modules for collecting, handling, and processing Twitter data, and you will be acquainted with them as we move along the course.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.8.1-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting click (from nltk)\n",
      "  Downloading click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: joblib in c:\\users\\mit\\documents\\pythonprojects\\env\\dlogger\\lib\\site-packages (from nltk) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\mit\\documents\\pythonprojects\\env\\dlogger\\lib\\site-packages (from nltk) (2023.8.8)\n",
      "Collecting tqdm (from nltk)\n",
      "  Downloading tqdm-4.66.4-py3-none-any.whl.metadata (57 kB)\n",
      "     ---------------------------------------- 0.0/57.6 kB ? eta -:--:--\n",
      "     ------- -------------------------------- 10.2/57.6 kB ? eta -:--:--\n",
      "     --------------------- ------------------ 30.7/57.6 kB 1.3 MB/s eta 0:00:01\n",
      "     --------------------------- ---------- 41.0/57.6 kB 326.8 kB/s eta 0:00:01\n",
      "     -------------------------------------- 57.6/57.6 kB 378.5 kB/s eta 0:00:00\n",
      "Requirement already satisfied: colorama in c:\\users\\mit\\documents\\pythonprojects\\env\\dlogger\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.1/1.5 MB 302.7 kB/s eta 0:00:05\n",
      "   - -------------------------------------- 0.1/1.5 MB 302.7 kB/s eta 0:00:05\n",
      "   - -------------------------------------- 0.1/1.5 MB 302.7 kB/s eta 0:00:05\n",
      "   --- ------------------------------------ 0.1/1.5 MB 313.8 kB/s eta 0:00:05\n",
      "   --- ------------------------------------ 0.1/1.5 MB 341.3 kB/s eta 0:00:05\n",
      "   ---- ----------------------------------- 0.2/1.5 MB 316.5 kB/s eta 0:00:05\n",
      "   ----- ---------------------------------- 0.2/1.5 MB 380.8 kB/s eta 0:00:04\n",
      "   ----- ---------------------------------- 0.2/1.5 MB 405.0 kB/s eta 0:00:04\n",
      "   ------ --------------------------------- 0.2/1.5 MB 389.8 kB/s eta 0:00:04\n",
      "   ------- -------------------------------- 0.3/1.5 MB 432.0 kB/s eta 0:00:03\n",
      "   -------- ------------------------------- 0.3/1.5 MB 447.2 kB/s eta 0:00:03\n",
      "   -------- ------------------------------- 0.3/1.5 MB 447.2 kB/s eta 0:00:03\n",
      "   -------- ------------------------------- 0.3/1.5 MB 447.2 kB/s eta 0:00:03\n",
      "   --------- ------------------------------ 0.3/1.5 MB 416.2 kB/s eta 0:00:03\n",
      "   ---------- ----------------------------- 0.4/1.5 MB 444.7 kB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 0.4/1.5 MB 452.1 kB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 0.5/1.5 MB 462.0 kB/s eta 0:00:03\n",
      "   ------------ --------------------------- 0.5/1.5 MB 471.0 kB/s eta 0:00:03\n",
      "   ------------- -------------------------- 0.5/1.5 MB 486.7 kB/s eta 0:00:03\n",
      "   -------------- ------------------------- 0.6/1.5 MB 503.4 kB/s eta 0:00:02\n",
      "   --------------- ------------------------ 0.6/1.5 MB 512.0 kB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 0.6/1.5 MB 526.2 kB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 0.7/1.5 MB 524.3 kB/s eta 0:00:02\n",
      "   ------------------ --------------------- 0.7/1.5 MB 544.8 kB/s eta 0:00:02\n",
      "   ------------------- -------------------- 0.7/1.5 MB 548.6 kB/s eta 0:00:02\n",
      "   -------------------- ------------------- 0.8/1.5 MB 558.5 kB/s eta 0:00:02\n",
      "   --------------------- ------------------ 0.8/1.5 MB 576.1 kB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 0.9/1.5 MB 572.7 kB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 0.9/1.5 MB 581.3 kB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 0.9/1.5 MB 581.3 kB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 0.9/1.5 MB 581.3 kB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 0.9/1.5 MB 581.3 kB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 1.1/1.5 MB 613.6 kB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 1.1/1.5 MB 608.5 kB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 1.1/1.5 MB 609.7 kB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 1.1/1.5 MB 610.6 kB/s eta 0:00:01\n",
      "   ------------------------------ --------- 1.2/1.5 MB 606.8 kB/s eta 0:00:01\n",
      "   ------------------------------- -------- 1.2/1.5 MB 608.0 kB/s eta 0:00:01\n",
      "   -------------------------------- ------- 1.2/1.5 MB 609.3 kB/s eta 0:00:01\n",
      "   -------------------------------- ------- 1.2/1.5 MB 605.0 kB/s eta 0:00:01\n",
      "   --------------------------------- ------ 1.3/1.5 MB 610.3 kB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 1.3/1.5 MB 602.4 kB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 1.3/1.5 MB 603.5 kB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 1.4/1.5 MB 604.5 kB/s eta 0:00:01\n",
      "   ------------------------------------ --- 1.4/1.5 MB 606.0 kB/s eta 0:00:01\n",
      "   ------------------------------------- -- 1.4/1.5 MB 606.3 kB/s eta 0:00:01\n",
      "   ------------------------------------- -- 1.4/1.5 MB 607.2 kB/s eta 0:00:01\n",
      "   ---------------------------------------  1.5/1.5 MB 612.7 kB/s eta 0:00:01\n",
      "   ---------------------------------------  1.5/1.5 MB 609.5 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.5/1.5 MB 604.0 kB/s eta 0:00:00\n",
      "Downloading click-8.1.7-py3-none-any.whl (97 kB)\n",
      "   ---------------------------------------- 0.0/97.9 kB ? eta -:--:--\n",
      "   ------------ --------------------------- 30.7/97.9 kB 1.4 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 61.4/97.9 kB 825.8 kB/s eta 0:00:01\n",
      "   --------------------------------- ------ 81.9/97.9 kB 762.6 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 97.9/97.9 kB 701.6 kB/s eta 0:00:00\n",
      "Downloading tqdm-4.66.4-py3-none-any.whl (78 kB)\n",
      "   ---------------------------------------- 0.0/78.3 kB ? eta -:--:--\n",
      "   --------------- ------------------------ 30.7/78.3 kB 640.0 kB/s eta 0:00:01\n",
      "   ------------------------------------ --- 71.7/78.3 kB 777.7 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 78.3/78.3 kB 723.8 kB/s eta 0:00:00\n",
      "Installing collected packages: tqdm, click, nltk\n",
      "Successfully installed click-8.1.7 nltk-3.8.1 tqdm-4.66.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.3.1 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in c:\\users\\mit\\documents\\pythonprojects\\env\\dlogger\\lib\\site-packages (3.6.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\mit\\documents\\pythonprojects\\env\\dlogger\\lib\\site-packages (from matplotlib) (1.0.6)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\mit\\documents\\pythonprojects\\env\\dlogger\\lib\\site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\mit\\documents\\pythonprojects\\env\\dlogger\\lib\\site-packages (from matplotlib) (4.38.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\mit\\documents\\pythonprojects\\env\\dlogger\\lib\\site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: numpy>=1.19 in c:\\users\\mit\\documents\\pythonprojects\\env\\dlogger\\lib\\site-packages (from matplotlib) (1.23.5)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\mit\\documents\\pythonprojects\\env\\dlogger\\lib\\site-packages (from matplotlib) (21.3)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\mit\\documents\\pythonprojects\\env\\dlogger\\lib\\site-packages (from matplotlib) (9.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\mit\\documents\\pythonprojects\\env\\dlogger\\lib\\site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\mit\\documents\\pythonprojects\\env\\dlogger\\lib\\site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\mit\\documents\\pythonprojects\\env\\dlogger\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.3.1 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import twitter_samples\n",
    "import matplotlib.pyplot as plt \n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**About the Twitter dataset**\n",
    "\n",
    "The sample dataset from NLTK is separated into positive and negative tweets. It contains 5000 positive tweets and 5000 negative tweets exactly. The exact match between these classes is not a coincidence. The intention is to have a balanced dataset. That does not reflect the real distributions of positive and negative classes in live Twitter streams. It is just because balanced datasets simplify the design of most computational methods that are required for sentiment analysis. However, it is better to be aware that this balance of classes is artificial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package twitter_samples to\n",
      "[nltk_data]     C:\\Users\\Mit\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package twitter_samples is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('twitter_samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'C:\\\\Users\\\\Mit\\\\Documents\\\\PythonProjects\\\\NLP\\\\nltk_data\\\\corpora\\\\twitter_samples\\\\'\n",
    "all_positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "all_negative_tweets = twitter_samples.strings('negative_tweets.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of positive tweets:  5000\n",
      "Number of negative tweets:  5000\n",
      "\n",
      "The type of all_positive_tweets is:  <class 'list'>\n",
      "The type of a tweet entry is:  <class 'str'>\n"
     ]
    }
   ],
   "source": [
    "print('Number of positive tweets: ', len(all_positive_tweets))\n",
    "print('Number of negative tweets: ', len(all_negative_tweets))\n",
    "\n",
    "print('\\nThe type of all_positive_tweets is: ', type(all_positive_tweets))\n",
    "print('The type of a tweet entry is: ', type(all_negative_tweets[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m@lovingjeonboram :) Lets take it lighter down, shall we? \n",
      "\n",
      "https://t.co/T74F2BtbRh\n",
      "\n",
      ":-)\n",
      "\u001b[91m@LuciHolland I completely forgot about Final Symphony :(\n",
      "Need to meet up again sometime soon with Joe and co.\n"
     ]
    }
   ],
   "source": [
    "# print positive in greeen\n",
    "print('\\033[92m' + all_positive_tweets[random.randint(0,5000)])\n",
    "\n",
    "# print negative in red\n",
    "print('\\033[91m' + all_negative_tweets[random.randint(0,5000)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preprocess raw text for Sentiment analysis**\n",
    "\n",
    "Data preprocessing is one of the critical steps in any machine learning project. It includes cleaning and formatting the data before feeding into a machine learning algorithm. For NLP, the preprocessing steps are comprised of the following tasks:\n",
    "\n",
    "1. Tokenizing the string\n",
    "2. Lowercasing\n",
    "3. Removing stop words and punctuation\n",
    "4. Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My beautiful sunflowers on a sunny Friday morning off :) #sunflowers #favourites #happy #Friday off… https://t.co/3tfYom0N1i\n"
     ]
    }
   ],
   "source": [
    "tweet = all_positive_tweets[2277]\n",
    "print(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Mit\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download the stopwords from NLTK\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import TweetTokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mMy beautiful sunflowers on a sunny Friday morning off :) #sunflowers #favourites #happy #Friday off… https://t.co/3tfYom0N1i\n",
      "\u001b[94m\n",
      "My beautiful sunflowers on a sunny Friday morning off :) #sunflowers #favourites #happy #Friday off… https://t.co/3tfYom0N1i\n",
      "My beautiful sunflowers on a sunny Friday morning off :) #sunflowers #favourites #happy #Friday off… \n",
      "My beautiful sunflowers on a sunny Friday morning off :) sunflowers favourites happy Friday off… \n"
     ]
    }
   ],
   "source": [
    "print('\\033[92m' + tweet)\n",
    "print('\\033[94m')\n",
    "\n",
    "# Remove old style retweet text \"RT\"\n",
    "tweet2 = re.sub(r'^RT[\\s]+', '', tweet)\n",
    "print(tweet2)\n",
    "\n",
    "# Remove hyperlinks\n",
    "tweet2 = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet2)\n",
    "print(tweet2)\n",
    "\n",
    "# Remove hashtags\n",
    "tweet2 = re.sub(r'#', '', tweet2)\n",
    "print(tweet2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tokenize the string**\n",
    "\n",
    "To tokenize means to split the strings into individual words without blanks or tabs. In this same step, we will also convert each word in the string to lower case. The tokenize module from NLTK allows us to do these easily:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mMy beautiful sunflowers on a sunny Friday morning off :) sunflowers favourites happy Friday off… \n",
      "['my', 'beautiful', 'sunflowers', 'on', 'a', 'sunny', 'friday', 'morning', 'off', ':)', 'sunflowers', 'favourites', 'happy', 'friday', 'off', '…']\n"
     ]
    }
   ],
   "source": [
    "print('\\033[92m' + tweet2)\n",
    "\n",
    "# Instance tokenizer class\n",
    "tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,\n",
    "                           reduce_len = True)\n",
    "\n",
    "# Tokenize tweets\n",
    "tweet_tokens = tokenizer.tokenize(tweet2)\n",
    "print(tweet_tokens)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remove stop words and punctuations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
      "\n",
      "Punctuation\n",
      "\n",
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "# Import the english stop words list from NLTK\n",
    "stopwords_english = stopwords.words('english')\n",
    "print(stopwords_english)\n",
    "\n",
    "print('\\nPunctuation\\n')\n",
    "print(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[92m\n",
      "['my', 'beautiful', 'sunflowers', 'on', 'a', 'sunny', 'friday', 'morning', 'off', ':)', 'sunflowers', 'favourites', 'happy', 'friday', 'off', '…']\n",
      "\u001b[94m\n",
      "after clening\n",
      "\n",
      "['beautiful', 'sunflowers', 'sunny', 'friday', 'morning', ':)', 'sunflowers', 'favourites', 'happy', 'friday', '…']\n"
     ]
    }
   ],
   "source": [
    "print()\n",
    "print('\\033[92m')\n",
    "print(tweet_tokens)\n",
    "print('\\033[94m')\n",
    "\n",
    "tweets_clean = []\n",
    "\n",
    "for word in tweet_tokens:\n",
    "    if(word not in stopwords_english and\n",
    "       word not in string.punctuation):\n",
    "        tweets_clean.append(word)\n",
    "        \n",
    "print(\"after clening\\n\")\n",
    "print(tweets_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK has different modules for stemming and we will be using the PorterStemmer module which uses the Porter Stemming Algorithm. Let's see how we can use it in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[92m\n",
      "['beautiful', 'sunflowers', 'sunny', 'friday', 'morning', ':)', 'sunflowers', 'favourites', 'happy', 'friday', '…']\n",
      "\u001b[94m\n",
      "Stemmed words\n",
      "['beauti', 'sunflow', 'sunni', 'friday', 'morn', ':)', 'sunflow', 'favourit', 'happi', 'friday', '…']\n"
     ]
    }
   ],
   "source": [
    "print()\n",
    "print('\\033[92m')\n",
    "print(tweets_clean)\n",
    "print('\\033[94m')\n",
    "\n",
    "# Instantiate stemming class\n",
    "stemmer = PorterStemmer() \n",
    "\n",
    "# Create an empty list to store \n",
    "tweets_stem = []\n",
    "\n",
    "for word in tweets_clean:\n",
    "    stem_word = stemmer.stem(word)\n",
    "    tweets_stem.append(stem_word)\n",
    "    \n",
    "\n",
    "print(\"Stemmed words\")\n",
    "print(tweets_stem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Utils Function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "\n",
    "def process_tweet(tweet):\n",
    "    \"\"\"Process tweet function.\n",
    "    Input:\n",
    "        tweet: a string containing a tweet\n",
    "    Output:\n",
    "        tweets_clean: a list of words containing the processed tweet\n",
    "\n",
    "    \"\"\"\n",
    "    stemmer = PorterStemmer()\n",
    "    stopwords_english = stopwords.words('english')\n",
    "    # remove stock market tickers like $GE\n",
    "    tweet = re.sub(r'\\$\\w*', '', tweet)\n",
    "    # remove old style retweet text \"RT\"\n",
    "    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
    "    # remove hyperlinks\n",
    "    tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet)\n",
    "    # remove hashtags\n",
    "    # only removing the hash # sign from the word\n",
    "    tweet = re.sub(r'#', '', tweet)\n",
    "    # tokenize tweets\n",
    "    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,\n",
    "                               reduce_len=True)\n",
    "    tweet_tokens = tokenizer.tokenize(tweet)\n",
    "\n",
    "    tweets_clean = []\n",
    "    for word in tweet_tokens:\n",
    "        if (word not in stopwords_english and  # remove stopwords\n",
    "                word not in string.punctuation):  # remove punctuation\n",
    "            # tweets_clean.append(word)\n",
    "            stem_word = stemmer.stem(word)  # stemming word\n",
    "            tweets_clean.append(stem_word)\n",
    "\n",
    "    return tweets_clean\n",
    "\n",
    "\n",
    "def build_freqs(tweets, ys):\n",
    "    \"\"\"Build frequencies.\n",
    "    Input:\n",
    "        tweets: a list of tweets\n",
    "        ys: an m x 1 array with the sentiment label of each tweet\n",
    "            (either 0 or 1)\n",
    "    Output:\n",
    "        freqs: a dictionary mapping each (word, sentiment) pair to its\n",
    "        frequency\n",
    "    \"\"\"\n",
    "    # Convert np array to list since zip needs an iterable.\n",
    "    # The squeeze is necessary or the list ends up with one element.\n",
    "    # Also note that this is just a NOP if ys is already a list.\n",
    "    yslist = np.squeeze(ys).tolist()\n",
    "\n",
    "    # Start with an empty dictionary and populate it by looping over all tweets\n",
    "    # and over all processed words in each tweet.\n",
    "    freqs = {}\n",
    "    for y, tweet in zip(yslist, tweets):\n",
    "        for word in process_tweet(tweet):\n",
    "            pair = (word, y)\n",
    "            if pair in freqs:\n",
    "                freqs[pair] += 1\n",
    "            else:\n",
    "                freqs[pair] = 1\n",
    "\n",
    "    return freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[92m\n",
      "My beautiful sunflowers on a sunny Friday morning off :) #sunflowers #favourites #happy #Friday off… https://t.co/3tfYom0N1i\n",
      "\u001b[94m\n",
      "preprocessed tweet:\n",
      "['beauti', 'sunflow', 'sunni', 'friday', 'morn', ':)', 'sunflow', 'favourit', 'happi', 'friday', '…']\n"
     ]
    }
   ],
   "source": [
    "# from utils import process_tweet # Import the process_tweet function\n",
    "\n",
    "# choose the same tweet\n",
    "tweet = all_positive_tweets[2277]\n",
    "\n",
    "print()\n",
    "print('\\033[92m')\n",
    "print(tweet)\n",
    "print('\\033[94m')\n",
    "\n",
    "# call the imported function\n",
    "tweets_stem = process_tweet(tweet); # Preprocess a given tweet\n",
    "\n",
    "print('preprocessed tweet:')\n",
    "print(tweets_stem) # Print the result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
